{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convolutional neural network archtitecture for binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#install all necessary libraries \n",
    "#pip install Keras\n",
    "#pip install scikit-plot\n",
    "#pip install -U scikit-learn\n",
    "#pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing all necessary libraries\n",
    "\n",
    "#general libraries\n",
    "import os\n",
    "\n",
    "#deep learning libraries \n",
    "import tensorflow as tf \n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, BatchNormalization, Conv2D, MaxPooling2D\n",
    "from keras.models import model_from_json\n",
    "from keras.models import load_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.constraints import maxnorm\n",
    "from keras.utils import np_utils\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from keras import optimizers\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "#data format libraries\n",
    "import pickle\n",
    "\n",
    "#visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import scikitplot as skplt\n",
    "\n",
    "#math libraries \n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, cohen_kappa_score, roc_auc_score, precision_recall_fscore_support\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check tensorflow version\n",
    "#print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set random seed for purposes of reproducibility\n",
    "seed = 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data\n",
    "X_train = pickle.load(open(\"/Users/there/Doktorand/Journal Paper/Paper BLM Reactions/Data/Classifier Data/final_Train Data/X_train.pickle\", \"rb\"))\n",
    "y_train = pickle.load(open(\"/Users/there/Doktorand/Journal Paper/Paper BLM Reactions/Data/Classifier Data/final_Train Data/y_train.pickle\", \"rb\"))\n",
    "\n",
    "X_test = pickle.load(open(\"/Users/there/Doktorand/Journal Paper/Paper BLM Reactions/Data/Classifier Data/final_Test Data/X_test.pickle\", \"rb\"))\n",
    "y_test = pickle.load(open(\"/Users/there/Doktorand/Journal Paper/Paper BLM Reactions/Data/Classifier Data/final_Test Data/y_test.pickle\", \"rb\"))\n",
    "\n",
    "#normalizing data in order to fit classifier dimensions later on\n",
    "X_train = X_train/255.0\n",
    "X_test = X_test/255.0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking pictures and data type \n",
    "print(X_train)\n",
    "print(type(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get shape of data\n",
    "print('X_train shape:', X_train.shape) # rows, pixel, pixel, depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print the label of the image\n",
    "print('the label is:', y_train[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#include batch normalization (normalizing the data after each layer) in order to prevent overfittting\n",
    "keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, \n",
    "                                beta_initializer='zeros', gamma_initializer='ones', \n",
    "                                moving_mean_initializer='zeros', moving_variance_initializer='ones', \n",
    "                                beta_regularizer=None, gamma_regularizer=None, beta_constraint=None, \n",
    "                                gamma_constraint=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#building the model\n",
    "#The first thing to do is define the format for the model. Keras has several different formats or blueprints \n",
    "#to build models on, but Sequential is the most commonly used, and best applicable one. Therefore, I use a \n",
    "#Sequential model. \n",
    "\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu',\n",
    "                        input_shape=(X_train.shape[1:])))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu')) \n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(layers.Dropout(0.1)) \n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu')) \n",
    "model.add(layers.MaxPooling2D((2, 2))) \n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dropout(0.4)) \n",
    "model.add(layers.Dense(512, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#look at model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compile the model with binary crossentropy and not sparse_categorical_crossentropy \n",
    "#because I aim for a binary classification\n",
    "\n",
    "model.compile(loss='binary_crossentropy', \n",
    "              optimizer=optimizers.RMSprop(learning_rate=1e-4),\n",
    "              metrics=['acc'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use these commands to convert labels to np array\n",
    "y_train = np.asarray(y_train)\n",
    "y_test = np.asarray(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model summary\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#include early stopping function to prevent overfitting\n",
    "#accuracy needs to improve at least 0.01% in 10 epochs in order to keep model running\n",
    "checkpoint_path= \"/Users/there/Doktorand/Journal Paper/Paper BLM Reactions/Python/CNNmodel.h5\"\n",
    "\n",
    "earlystop_callback = [\n",
    "      EarlyStopping(monitor='val_loss', patience=10, mode='min', min_delta=0.0001),\n",
    "      ModelCheckpoint(checkpoint_path, monitor='val_loss', save_best_only=True, mode='min')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#overfitting might be a problem, therefore I included measures to prevent overfitting\n",
    "#included options in order to avoid overfitting are:\n",
    "\n",
    "#batch normalization\n",
    "#add dropout layers \n",
    "#increase the dataset by augmentation\n",
    "#use batch size as large as possible (64)\n",
    "#increase the layers in neural network\n",
    "#use callback functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training the model, with 40 iterations by also splitting the data in 70% training data and 30% validation data\n",
    "\n",
    "np.random.seed(seed) #random seed for reproducability \n",
    "history = model.fit(X_train, y_train, validation_split=0.3, batch_size=64, epochs=40, verbose=1, callbacks=earlystop_callback) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving the model\n",
    "model_json = model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file :\n",
    "    json_file.write(model_json)\n",
    "\n",
    "model.save_weights(\"model12.h5\")\n",
    "print(\"Saved model to disk\")\n",
    "\n",
    "model.save('CNN12.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluating the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load model\n",
    "model = tf.keras.models.load_model(\"/Users/there/Doktorand/Journal Paper/Paper BLM Reactions/Python/CNNmodel.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#look at optimizer\n",
    "model.optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#look at model weights\n",
    "model.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#printing a graph showing the accuracy changes during the training phase\n",
    "#print(history.history.keys())\n",
    "#plt.figure(1)\n",
    "#plt.plot(history.history['acc'])\n",
    "#plt.plot(history.history['val_acc'])\n",
    "#plt.title('model acc')\n",
    "#plt.ylabel('accuracy')\n",
    "#plt.xlabel('epoch')\n",
    "#plt.legend(['train', 'validation'], loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize the loss function of model\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(len(acc))\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate the model's performance with test data\n",
    "#this is a dataset the model has never seen before; this is another good check for overfitting \n",
    "scores = model.evaluate(X_test, y_test, verbose=1)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicting values\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#round values in order to avoid decimal numbers\n",
    "y_pred = np.where(y_pred > 0.5, 1,0)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#look at rounded values\n",
    "for i in y_pred:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#confusion matrix as another check\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot absolute confucion matrix \n",
    "skplt.metrics.plot_confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#plot relative confusion matrix \n",
    "skplt.metrics.plot_confusion_matrix(y_test, y_pred, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#precision score \n",
    "precision_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#recall score\n",
    "recall_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#F1 score\n",
    "f1_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cohen's Kappa score\n",
    "cohen_kappa_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#precision, recall, fscore, count of each category \n",
    "precision_recall_fscore_support(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ROC Curve value \n",
    "roc_auc_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting roc curve \n",
    "fpr, tpr, thresholds = skplt.metrics.roc_curve(y_test,  y_pred) # false positve rate and true positive rate \n",
    "auc = roc_auc_score(y_test, y_pred)\n",
    "plt.plot(fpr,tpr,label=\"ROC Curve, AUC=\"+str(auc))\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label=\"Random chances\")\n",
    "plt.legend(loc=4)\n",
    "plt.title(\"Receiver Operating Characteristic (ROC) Curve\")\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.show()\n",
    "print(auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#look at false positive rate\n",
    "fpr"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
