{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download specific submissions from Reddit defiend by parameters after, before, and title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import all important libraries\n",
    "\n",
    "#general libraries\n",
    "import os \n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "#data format libraries\n",
    "import json\n",
    "import csv\n",
    "\n",
    "#time libraries\n",
    "import datetime as dt\n",
    "import time\n",
    "\n",
    "#API connection\n",
    "from pmaw import PushshiftAPI\n",
    "import requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define function for downloading submmissions with specific parameters\n",
    "def getPushshiftData(after, before, title, limit):\n",
    "    url = 'https://api.pushshift.io/reddit/search/submission/?title='+str(title)+'&after='+str(after)+'&before='+str(before)+'&limit='+str(limit)\n",
    "    print(url)\n",
    "    r = requests.get(url)\n",
    "    data = json.loads(r.text)\n",
    "    return data['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set parameters according to own preferences\n",
    "title=\"(black%20lives%20matter)|(george%20floyd)\" #search strings in title\n",
    "before = \"1621979999\" #May 25th 2021\n",
    "after = \"1590357600\"  #May 25th 2020 --> George Floyds death date    \n",
    "limit = \"1000\" #returns 1000 items each query\n",
    "#subData = []\n",
    "subCount = 0\n",
    "subStats = {}\n",
    "            \n",
    "            \n",
    "#get submission request with specified parameters\n",
    "data = getPushshiftData(after, before, title, limit)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#funciton for collecting all necessary information for one submission  \n",
    "#add try and except statement for preventing code failure in case information is not provided\n",
    "def collectSubData(subm):\n",
    "    subData = list()\n",
    "    title = subm['title']\n",
    "    url = subm['url'] \n",
    "    author = subm['author']\n",
    "    try:\n",
    "        author_fullname = subm['author_fullname']\n",
    "    except KeyError:\n",
    "        author_fullname = \"NaN\"\n",
    "    try:\n",
    "        author_premium = subm['author_premium']\n",
    "    except KeyError:\n",
    "        author_premium = \"NaN\"\n",
    "    try:\n",
    "        domain = subm['domain']\n",
    "    except KeyError:\n",
    "        domain = \"NaN\"\n",
    "    try:\n",
    "        selftext = subm['selftext']\n",
    "    except KeyError:\n",
    "        selftext = \"NaN\"\n",
    "    sub_id = subm['id']\n",
    "    subreddit_subscribers = subm['subreddit_subscribers']\n",
    "    score = subm['score']\n",
    "    upvote_ratio = subm['upvote_ratio']\n",
    "    subreddit = subm['subreddit']\n",
    "    subreddit_id = subm['subreddit_id']\n",
    "    created = datetime.datetime.fromtimestamp(subm['created_utc'])\n",
    "    retrieved = datetime.datetime.fromtimestamp(subm['retrieved_on'])\n",
    "    numComms = subm['num_comments']\n",
    "    num_crossposts = subm['num_crossposts']\n",
    "    permalink = subm['permalink']\n",
    "    full_link = subm['full_link']\n",
    "    thumbnail = subm['thumbnail']\n",
    "    try:\n",
    "        pinned = subm['pinned']\n",
    "    except KeyError:\n",
    "        pinned = \"NaN\"\n",
    "    try:\n",
    "        stickied = subm['stickied']\n",
    "    except KeyError:\n",
    "        stickied = \"NaN\"\n",
    "    try:\n",
    "        is_video = subm['is_video']\n",
    "    except KeyError:\n",
    "        is_video = \"NaN\"\n",
    "    try:\n",
    "        media_only = subm['media_only']\n",
    "    except KeyError:\n",
    "        media_only = \"NaN\"\n",
    "    try:\n",
    "        over_18 = subm['over_18']\n",
    "    except KeyError:\n",
    "        over_18 = \"NaN\"\n",
    "    try:\n",
    "        is_reddit_media_domain = subm['is_reddit_media_domain']\n",
    "    except KeyError:\n",
    "        is_reddit_media_domain = \"NaN\"\n",
    "    try:\n",
    "        is_original_content = subm['is_original_content']\n",
    "    except KeyError:\n",
    "        is_original_content = \"NaN\"\n",
    "    try:\n",
    "        is_crosspostable = subm['is_crosspostable']\n",
    "    except KeyError:\n",
    "        is_crosspostable = \"NaN\"\n",
    "    try:\n",
    "        link_flair_text = subm['link_flair_text']\n",
    "    except KeyError:\n",
    "        link_flair_text = \"NaN\"\n",
    "    try:\n",
    "        link_flair_type = subm['link_flair_type']\n",
    "    except KeyError:\n",
    "        link_flair_type = \"NaN\"\n",
    "    try:\n",
    "        total_awards_received = subm['total_awards_received']\n",
    "    except KeyError:\n",
    "        total_awards_received = \"NaN\"\n",
    "    try:\n",
    "        media = subm['media']\n",
    "    except KeyError:\n",
    "        media = \"NaN\"\n",
    "    \n",
    "    \n",
    "    #append all data to list\n",
    "    subData.append((sub_id,title,url,author,author_fullname,author_premium,selftext,subreddit,subreddit_id,\\\n",
    "                    subreddit_subscribers,domain,score,upvote_ratio,created,retrieved,numComms,num_crossposts,\\\n",
    "                    permalink,full_link,thumbnail,pinned,stickied,is_video,media_only,over_18,\\\n",
    "                    is_reddit_media_domain,is_original_content,is_crosspostable,link_flair_text,link_flair_type,\\\n",
    "                    total_awards_received,media))\n",
    "    subStats[sub_id] = subData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#create while loop that will run until all posts in specific time frame have been collected \n",
    "\n",
    "while len(data) > 0:\n",
    "    for submission in data:\n",
    "        collectSubData(submission)\n",
    "        subCount+=1\n",
    "    \n",
    "    #calls getPushshiftData() with the created date of the last submission\n",
    "    print(len(data))\n",
    "    print(str(datetime.datetime.fromtimestamp(data[-1]['created_utc'])))\n",
    "    after = data[-1]['created_utc']\n",
    "    try:\n",
    "        data = getPushshiftData(after, before, title, limit)\n",
    "    except Exception as e:\n",
    "        print(\"Error\")\n",
    "    \n",
    "    \n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#turn the dictionary values into a list and show first and last entry\n",
    "print(str(len(subStats)) + \" submissions have added to list\")\n",
    "print(\"1st entry is:\")\n",
    "print(list(subStats.values())[0][0][1] + \" created: \" + str(list(subStats.values())[0][0][5]))\n",
    "print(\"Last entry is:\")\n",
    "print(list(subStats.values())[-1][0][1] + \" created: \" + str(list(subStats.values())[-1][0][5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save data to CSV file\n",
    "def updateSubs_file():\n",
    "    upload_count = 0\n",
    "    location = \"/Users/there/Doktorand/Journal Paper/Paper BLM Reactions/Data\"\n",
    "    print(\"input filename of submission file, please add .csv\")\n",
    "    filename = input()\n",
    "    file = location + filename\n",
    "    with open(file, 'w', newline='', encoding='utf-8') as file: \n",
    "        a = csv.writer(file, delimiter=';')\n",
    "        headers = [\"sub_id\",\"title\",\"url\",\"author\",\"author_fullname\",\"author_premium\",\"selftext\",\"subreddit\",\\\n",
    "                   \"subreddit_id\",\"subreddit_subscribers\",\"domain\",\"score\",\"upvote_ratio\",\"created\",\\\n",
    "                   \"retrieved\",\"number_of_comments\",\"number_crossposts\",\"permalink\",\"full_Link\",\"thumbnail\",\\\n",
    "                   \"pinned\",\"stickied\",\"is_video\",\"media_only\",\"over_18\",\"is_reddit_media_domain\",\\\n",
    "                   \"is_original_content\", \"is_crosspostable\",\"link_flair_text\", \"link_flair_type\",\\\n",
    "                   \"total_awards_received\",\"media\"]\n",
    "        a.writerow(headers)\n",
    "        for sub in subStats:\n",
    "            a.writerow(subStats[sub][0])\n",
    "            upload_count+=1\n",
    "            \n",
    "        print(str(upload_count) + \" submissions have been uploaded\")\n",
    "updateSubs_file()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
